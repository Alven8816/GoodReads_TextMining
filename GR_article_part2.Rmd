---
title: "GoodReads: webscraping and text analysis with R: part 2"
author: "Florent Buisson"
date: "August 17, 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

After scraping reviews from Goodreads in the first installment of this series, we are now ready to do some exploratory data analysis to get a better sense of the data we have. This will also allow us to create features that we will use in future analyses

## Part 2: Exploratory data analysis and sentiment analysis

# Setup and data preparation

We start by loading the libraries and the data from part 1, that I have consolidated in one file.

```{r, eval=FALSE}
library(data.table)
library(dplyr)
library(ggplot2)
library(stringr)
library(tm)
library(magrittr)
library(textcat)
library(tidytext)
library(RTextTools)
```

```{r, eval=FALSE}
data = read.csv("GoodReadsData.csv", stringsAsFactors = FALSE)
data=data.table(data)
```

After a quick inspection of the data, we realize that some reviews are not in english. We get rid of them. 

```{r, eval=FALSE}
data$language=as.factor(textcat(data$review))
data=data[language=="english"]
```

The language detection algorithm is not great and reclassifies some reviews in weird languages (6 breton? 9 rumantsch?? 103 middle frisian???), but that's good enough for what we want to do. 

We then exclude all non-standard ratings and remove all reviews that are too short.

```{r, eval=FALSE}
data=data[rating %in% c('did not like it','it was ok','liked it','really liked it','it was amazing')]
data=data[length(data$review)>=5]
```

Finally, we recode the ratings in numerical format, remove the language and author columns which are useless for now, and add a review_id column (to be able to identify to which review a word belongs).

```{r, eval=FALSE}
data$rating[data$rating=='did not like it']=1
data$rating[data$rating=='it was ok']=2
data$rating[data$rating=='liked it']=3
data$rating[data$rating=='really liked it']=4
data$rating[data$rating=='it was amazing']=5
data$rating=as.integer(data$rating)

data$language=NULL
data$author=NULL
data$review_id=1:nrow(data)
```

With that, we are now ready to start exploratory data analysis!

# Exploratory data analysis

Let's start by looking at the distribution of ratings. As we can see, the ratings are rather unbalanced, something to keep in mind in future analyses.

```{r, eval=FALSE}
barplot(table(as.factor(data$rating)),ylim = c(0,5000), main = "Distribution of ratings")
```

![](Barplot1.png)

Let's take a look at the distribution of the length of reviews.

```{r, eval=FALSE}
data$review_length = nchar(data$review)
hist(data$review_length, ylim = c(0,5000), main = "Distribution of review length" )
```

![](Histogram1.png)

Now that's a long tail! A quick calculation let us know that there are only 45 reviews that are more than 8000 character long. Let's get rid of them, to avoid skewing our analyses (e.g. if one of these reviews uses a lot a word, it could bias the weight for that word).

```{r, eval=FALSE}
n=nrow(data[data$review_length>=8000])
data=data[data$review_length<=8000]
hist(data$review_length, ylim = c(0,3000), main = "Distribution of review length" )

```

![](Histogram2.png)

This looks better. Finally, let's take a look at the distribution of review length by rating. 

```{r, eval=FALSE}
with(data, boxplot(review_length~rating, main = "Distribution of review length by rating"))
```

![](Boxplot1.png)

Visually, more positive reviews appear to be slightly shorter than more negative reviews, but there's no definite trend. Let's turn to sentiment analysis, by replicating *mutatis mutandis* [the analyses of David Robinson on Yelp's reviews](https://www.r-bloggers.com/does-sentiment-analysis-work-a-tidy-analysis-of-yelp-reviews/) using the *tidytext* package.

# Sentiment analysis

In this section, we are going to use the "positive" or "negative" aspect of words to see if it correlates with the ratings. In order to do that, we need to start by establishing a lexicon of words with a positive/negative score. 

```{r, eval=FALSE}
AFINN <- sentiments %>%
  filter(lexicon == "AFINN") %>%
  select(word, afinn_score = score)
head(AFINN)
```

We then "tidy up" our dataset by making it "tall" (one word per row).

```{r, eval=FALSE}
# "tidying" up the data (1 word per row)
review_words <- data %>%
  unnest_tokens(word, review)
```

Our data now looks like this:

book           | rating | review_id | word         |
---------------|--------|-----------|--------------|
Eleanor & Park | 5      | 1         | experienced  |
Eleanor & Park | 5      | 1         | love         |
Eleanor & Park | 5      | 1         | heart        |

We can assign a positivity/negativity "score" to each review by looking up in the lexicon the score of all the words in a review and calculating the average score for the review.

```{r, eval=FALSE}
review_mean_sentiment <- review_words %>%
  inner_join(AFINN, by = "word") %>%
  group_by(review_id, rating) %>%
  summarize(mean_sentiment = mean(afinn_score))
```

The outcome looks like this:

review_id | rating | sentiment |
----------|--------|-----------|
 1        | 5      | 0.9444    |
 2        | 3      | -0.093    |
 3        | 5      | -1.17     |

So, how does the average sentiment score vary by rating?

```{r, eval=FALSE}
theme_set(theme_bw())
ggplot(review_mean_sentiment, aes(rating, mean_sentiment, group = rating)) +
  geom_boxplot() +
  ylab("Average sentiment score")
```

![](Boxplot_mean_sentiment.png)

We're onto something! Visually at least, we can see a difference across ratings, with the sentiment score for 1-star reviews being squarely negative and the sentiment score for 5-star reviews being squarely positive. Let's create a new dataset to integrate this feature for future use (note that in the process we lose a few observations, reviews that are too short to get an AFINN score).

```{r, eval = FALSE}
review_mean_sentiment <- review_mean_sentiment %>%
  select(-rating) %>% # We remove the rating here to avoid duplicating it
  data.table()
clean_data <- data %>%
  left_join(review_mean_sentiment, by = "review_id")
```

The difference between ratings is even clearer if we take the median score instead of the mean:

```{r, eval=FALSE}
review_median_sentiment <- review_words %>%
  inner_join(AFINN, by = "word") %>%
  group_by(review_id, rating) %>%
  summarize(median_sentiment = median(afinn_score))
theme_set(theme_bw())
ggplot(review_median_sentiment, aes(rating, median_sentiment, group = rating)) +
  geom_boxplot() +
  ylab("Median sentiment score")
```

![](Boxplot_median_sentiment.png)

Again, let's transfer this feature to our new dataset.

```{r, eval = FALSE}
review_median_sentiment <- review_median_sentiment %>%
  select(-rating) %>%
  data.table()
clean_data <- clean_data %>%
  left_join(review_median_sentiment, by = "review_id")
```

Finally, we save our data to a file for future use.

```{r, eval = FALSE}
write.csv(clean_data, "GoodReadsCleanData.csv", row.names = FALSE)
```

Just for exploratory purposes, we are now going to slice our data the other way around, aggregating not by review but by word. First, for each word, we are going to count in how many reviews it appears and how many times it appears overall, as well as calculate the average rating of the reviews in which it appears. Finally, we filter our data to keep only the words that appears in at least 3 reviews, to avoid words that would be peculiar to a specific reviewer.

```{r, eval=FALSE}
word_mean_summaries <- review_words %>%
  count(review_id, rating, word) %>%
  group_by(word) %>%
  summarize(reviews = n(),
            uses = sum(n),
            average_rating = mean(rating)) %>%
  filter(reviews >= 3) %>%
  arrange(average_rating)
```

The outcome looks like this:

word       | reviews | uses | average_rating
-----------|---------|------|----------------
mystified  | 3       | 3    | 1.6667
operator   | 3       | 3    | 1.6667
unlikeable | 9       | 12   | 1.6667

And finally, we can compare the average rating in the previous table with the AFINN score for the word: 

```{r, eval=FALSE}
word_mean_afinn <- word_mean_summaries %>%
  inner_join(AFINN)

ggplot(word_mean_afinn, aes(afinn_score, average_rating, group = afinn_score)) +
  geom_boxplot() +
  xlab("AFINN score of word") +
  ylab("Mean rating of reviews with this word")
```

![](Boxplot_mean_AFINN.png)

Once again, we can see that there is some correlation between the ratings and the AFINN scores, this time at the word level. The question is then: can we predict at least somewhat accurately the rating of a review based on the words of the review? That will be the topic of the last installment in this series.

As for the first installment, the complete R code is available on [my GitHub](https://github.com/BuissonFlorent/GoodReads_TextMining).
